{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirements: nltk, tensorflow, keras, tweepy, numpy, wikipedia\n",
    "#Importing and Pre-processing data!\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['.', ',', '?', '!', '#', '@']\n",
    "data_file = open('intents.json').read()\n",
    "intents = json.loads(data_file)\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "\n",
    "        #Splitting sentences into smaller phrases!\n",
    "        wo = nltk.word_tokenize(pattern)\n",
    "        words.extend(wo)\n",
    "        documents.append((w, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "#lower casing each word and grouping similar meaning words for the CNN\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "classes = sorted(list(set(classes)))\n",
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating training and testing data!\n",
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "#Splitting data into X and Y planes!\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print(\"Training data has been created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a CNN with 3 layers: first-128, second-64, final-output layer\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "#fitting and saving the model \n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=4, verbose=1)\n",
    "model.save('model.h5', hist)\n",
    "\n",
    "print(\"trained model completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the model\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import pickle\n",
    "import wikipedia\n",
    "from keras.models import load_model\n",
    "import json\n",
    "import random\n",
    "\n",
    "model = load_model('model.h5')\n",
    "intents = json.loads(open('intents.json').read())\n",
    "words = pickle.load(open('words.pkl','rb'))\n",
    "classes = pickle.load(open('classes.pkl','rb'))\n",
    "\n",
    "\n",
    "def clean_up(sentence): #function to link similar words to one word!\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "\n",
    "def bow(sentence, words, show_details=True):\n",
    "    sentence_words = clean_up(sentence)\n",
    "    bag = [0]*len(words)  \n",
    "    for i in sentence_words:\n",
    "        for j,k in enumerate(words):\n",
    "            if k == i: \n",
    "                bag[j] = 1\n",
    "                if show_details:\n",
    "                    print (\"found: %s\" % k)\n",
    "    return(np.array(bag))\n",
    "\n",
    "def predict_class(sentence, model):\n",
    "    p = bow(sentence, words,show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def get_time():\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M\")\n",
    "    time = \"The time is: \"+current_time\n",
    "    return time\n",
    "\n",
    "def chatbot_response(msg):\n",
    "    ints = predict_class(msg, model)\n",
    "    res = getResponse(ints, intents)\n",
    "    return res\n",
    "\n",
    "def wiki_data(msg):\n",
    "    result = wikipedia.summary(msg, sentences = 2) \n",
    "    result = str(result)\n",
    "    return result\n",
    "    \n",
    "def weather_data():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to twitter api\n",
    "import tweepy\n",
    "from tweepy import *\n",
    "\n",
    "#Access requiements to connect to twitter api!\n",
    "token1 = \"\"\n",
    "token2 = \"\"\n",
    "token3 = \"\"\n",
    "token4 = \"\"\n",
    "auth = tweepy.OAuthHandler(token1, token2)\n",
    "auth.set_access_token(token3, token4)\n",
    "# Creating API handler\n",
    "api = tweepy.API(auth)\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"API Verified!\")\n",
    "except:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the chatbot response\n",
    "import time\n",
    "\n",
    "name = \"LuqmaanKamaar\" #Inserting my user name\n",
    "user = api.get_user(name) #Getting user name as seen by api\n",
    "ID = user.id_str  #Getting the ID of my username\n",
    "time.sleep(20) #Waiting for tweepy to reload in order to list current messages!\n",
    "My_messages = api.list_direct_messages(count=5) #Getting direct messages from api\n",
    "for messages in reversed(My_messages): #looping through the list from most recent to older\n",
    "    sender_Id = messages.message_create[\"sender_id\"] #Storing the ID of the recent sender\n",
    "    text = messages.message_create[\"message_data\"][\"text\"] #Storing the text of the recent sender\n",
    "print(sender_Id, text)#Printing the ID and the actual message of the last received DM \n",
    "\n",
    "if 'What is the time' in text.lower(): #determining if the user is asking for the time\n",
    "    api.send_direct_message(sender_Id, get_time())\n",
    "#elif '1395793448310124548' == sender_Id:\n",
    "#    api.destroy_direct_message('1395793448310124548')\n",
    "else:\n",
    "    api.send_direct_message(sender_Id, chatbot_response(text)) #Otherwise, implementing the trained chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
